# literature_revision

	最近在忙着写毕业论文，写下来有几万字。相比于写的过程，检查的过程更令人感到繁琐。尤其是在错别字、语病和标点符号方面，检查不仅耗时费力，而且极易遗漏。。想用大模型，但是又不希望自己的论文被上传到互联网上。这时候就想，如果有个本地大模型能帮我看论文该有多好！奈何本人是学生党，没有钱也没有地方购买能跑大模型的GPU。因此我就想，能不能在云服务器上运行大模型，这样文档都保留在服务器上，不会上传至互联网，满足保密性的需求。

最近阿里新推出的QwQ推理模型。它仅有32b参数，而且官方说其能达到deepseek-r1:671b的水平，感觉很适合用来部署私有大模型。

	基本思路是：租用云服务器，用ollama运行本地大模型，再将论文上传至云服务器，用脚本调用大模型来帮我阅读并检查论文。

	首先需要选一个安全性良好、信誉较高的云服务器平台。这里我选择了国家超算互联网（https://www.scnet.cn/ui/mall/）。然后，点击“控制台”，创建一个notebook，租用一块A800 GPU，模型镜像选择jupyterlab-ollama，这样免去安装Ollama这一步了。

	进入实例，运行Ollama（在命令行中输入Ollama serve）。然后下载了QwQ，它在A800上运行非常顺畅，能达到几十个token/s的输出速度，而且感觉效果确实和deepseek-r1相当。

	Ollama run qwq

	接下来，将论文上传至云服务器。这里还需要安装docx2txt，将word转化为txt

	sudo apt-get install docx2txt
	docx2txt thesis.docx - > thesis.txt  # 将内容转换为纯文本并保存到 output.txt

	接下来，我首先尝试用curl命令，通过prompt把论文输入大模型。

curl -X POST http://localhost:11434/api/generate -d '{ "model": "qwq", "prompt": "请检查以下文本中的错别字，并列出出错的地方：\n\n'$(cat thesis.txt)'", "stream": false }' | jq -r '.response' > advice.txt

正常运行，但是输出的结果却并不是为论文纠错，而是一些对论文的总结，内容也不全，只有论文结尾部分。这时我想到，论文有三万多字，可能是因为prompt太长，大模型读到后面，就忘记前面的内容，忘记自己是要给论文纠错了。

这时候我求助了大模型gemini-flash-thinking，它说可以用chunking 的方法，就是把长论文分成很多切片，然后分别把它们输入大模型。Chunking我使用的是字符数量的固定大小的方法。让gemini帮我写脚本，之后主要就是运行-调试-运行-调试，主要调整的参数是切片的数量。切片太少，模型还是会忘记自己的任务（修改论文）；切片太多，运行时间太长，而且输出内容太多影响阅读。

经过几轮调试，终于找到了比较合适的切片数量，每个切片大概在800目标字符的时候，效果是比较好的。

最后它修改的效果甚至比我预想的还要好。它可以告诉我哪些地方有错别字（“阳离子”与“阳例子”），哪些地方多了一个“的”，还能告诉你标点符号是否正确，以及一些写作用语是否规范，语序是否需要调整等等。让它帮我修改完后，我给导师提交论文草稿的时候也更有信心啦！

如果有正在和我一样写毕业论文的同学，或者写学术论文的同学，感兴趣的话可以试试在服务器上搭建本地大模型作为你的私人“写作秘书”。完全不需要编程能力，本人也不是计算机系的学生。所有的代码和方法，都是通过跟大模型聊天现学现用的。

最终我使用的脚本也放在script.py里面，供大家参考。这里面的参数还需要大家根据自己论文的实际情况进行调整。




